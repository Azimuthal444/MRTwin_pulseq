#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{wasysym}
\usepackage{float}
\let\chapter\section
\usepackage{bm,colortbl}
\usepackage[round,numbers,comma,sort&compress]{natbib}

\usepackage[nomarkers]{endfloat} % all figures at the end
\let\MYoriglatexcaption\caption
\renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
\makeatletter
\def\markboth#1#2{\def\leftmark{\@IEEEcompsoconly{\sffamily}\MakeUppercase{\protect#1}}%
\def\rightmark{\@IEEEcompsoconly{\sffamily}\MakeUppercase{\protect#2}}}
\makeatother
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing onehalf
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Learning how to scan
\end_layout

\begin_layout Subsection
Model (spatial encoding only)
\end_layout

\begin_layout Standard
The following forward process describes the image acquisition process in
 the presence of the additive magnetic fields produced by the linear gradients:
\end_layout

\begin_layout Standard
\begin_inset Formula ${\displaystyle \boldsymbol{s}(\boldsymbol{r},t)=\boldsymbol{m}(\boldsymbol{r},t_{1})*exp(-\frac{t}{T2})*exp(i\int_{t_{1}}^{Ta}(G_{x}(\boldsymbol{r},t)+G_{y}(\boldsymbol{r},t))dt)}$
\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "Eq1.  continuous model"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace smallskip
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula ${\displaystyle G_{x}(\boldsymbol{r},t)=g_{x}(t)*ramp_{x}(\boldsymbol{r})\quad\quad\quad G_{y}(\boldsymbol{r},t)=g_{y}(t)*ramp_{y}(\boldsymbol{r})}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace smallskip
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula ${\displaystyle ramp_{x}(x,y)=x\quad\quad\quad\quad\quad ramp_{y}(x,y)=y}$
\end_inset


\end_layout

\begin_layout Standard
We assume the acquisition with a single receive coil element, which has
 a unity-valued sensitivity at each location of the imaged FoV.
 We denote 
\begin_inset Formula $\boldsymbol{m}(\boldsymbol{r},t_{1})$
\end_inset

 the function that describes the magnetization state of the system at spatial
 location 
\begin_inset Formula $\boldsymbol{r}$
\end_inset

 and time point 
\begin_inset Formula $t_{1}$
\end_inset

 , which is assumed to be a starting point of applying the gradients.
 Further, let scalar T2 denote the relaxation times, which are assumed to
 be constant at each spatial location.
 The model is trivially extended to spatially-varying relaxation times case
 by replacing the scalars with functions 
\begin_inset Formula $T2(\boldsymbol{r})$
\end_inset

.
 The final time point of the ADC readout is denoted as 
\begin_inset Formula $T_{a}$
\end_inset

.
 
\begin_inset Formula $ramp_{x/y}$
\end_inset

 define the spatial gradient form, which we assume to be linear in this
 case.
 Finally, at each time point we drive the gradient system with a current
 
\begin_inset Formula $g_{x}(t)$
\end_inset

 .
\end_layout

\begin_layout Standard
We can discretize the continuous model to obtain 
\begin_inset Formula $\boldsymbol{s}=\boldsymbol{E}(\boldsymbol{D}\boldsymbol{g}_{x},\boldsymbol{D}\boldsymbol{g}_{y})\boldsymbol{m}$
\end_inset

.
 Note that the exponential terms are packed into a matrix 
\begin_inset Formula $\boldsymbol{E}\in\mathbb{C}^{T\times N}$
\end_inset

, where 
\begin_inset Formula $N=N_{x}N_{y}$
\end_inset

 is the number of pixels in spatial domain, and 
\begin_inset Formula $T$
\end_inset

 total number of time points.
 Further, 
\begin_inset Formula $\boldsymbol{D}$
\end_inset

 is the lower triangular matrix filled with ones.
 The matrix 
\begin_inset Formula $\boldsymbol{D}$
\end_inset

 is used to compute the cumulative sum in the first dimension of the vector
 
\begin_inset Formula $\boldsymbol{g}$
\end_inset

, which is a counterpart of the integral in continous case 
\begin_inset CommandInset ref
LatexCommand ref
reference "Eq1.  continuous model"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Subsection
Optimization
\end_layout

\begin_layout Standard
The signal acquired at repetition 
\shape italic
p 
\shape default
is denoted as 
\begin_inset Formula $\boldsymbol{s}_{p}(\boldsymbol{r},t)$
\end_inset

 and is subject to repetition-dependent temporal gradient forms 
\begin_inset Formula $\boldsymbol{g}_{x,p}$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{g}_{y,p}$
\end_inset

.
 Let the matrix 
\begin_inset Formula $\boldsymbol{F}$
\end_inset

 be an orthonormal discrete Fourier operator.
 We seek to find the target image 
\begin_inset Formula $\boldsymbol{u}=\sum_{p}^{P}\boldsymbol{F}^{H}\boldsymbol{Q_{p}s_{p}}$
\end_inset

, where the operator 
\begin_inset Formula $\boldsymbol{Q}_{p}$
\end_inset

 insert the Fourier coefficients from readout 
\shape italic
p 
\shape default
into respective k-space locations.
 In order to learn the gradients that allow to approximate the target image
 
\begin_inset Formula $\boldsymbol{u}$
\end_inset

 as close as possible we use the following algorithm:
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement h
wide false
sideways false
status open

\begin_layout Plain Layout

\series bold
Input:
\series default
 Target image 
\begin_inset Formula $\boldsymbol{u}$
\end_inset

 of size 
\begin_inset Formula $N=N_{x}\cdot N_{y}$
\end_inset

; 
\begin_inset Formula $ramp_{x/y}$
\end_inset

 spatial gradient forms; 
\begin_inset Formula $P$
\end_inset

 total number of repetitions.
\end_layout

\begin_layout Plain Layout

\series bold
Output:
\series default
 Temporal gradient forms 
\begin_inset Formula $\boldsymbol{g}_{x/y,p}$
\end_inset

 at each repetition.
\end_layout

\begin_layout Itemize
Initialize 
\begin_inset Formula $\boldsymbol{\tilde{u}_{1}=}\boldsymbol{u}$
\end_inset

: the target to optimize at sequence repetition
\shape italic
 
\begin_inset Formula $p=1$
\end_inset

.
\end_layout

\begin_layout Itemize
Initialize 
\shape italic

\begin_inset Formula $\boldsymbol{w}=0$
\end_inset

: 
\shape default
the reconstructed image.
\end_layout

\begin_layout Itemize
Assume 
\begin_inset Formula ${\displaystyle \boldsymbol{E_{g}=}exp(i(\boldsymbol{D}\boldsymbol{g}_{x}ramp_{x}^{T}+\boldsymbol{D}\boldsymbol{g}_{y}ramp_{y}^{T}))}$
\end_inset


\end_layout

\begin_layout Plain Layout

\series bold
For
\series default
 
\begin_inset Formula $p\leftarrow1...P$
\end_inset

 
\series bold
do
\end_layout

\begin_layout Itemize
Find gradient forms 
\begin_inset Formula $\boldsymbol{g}_{x},\boldsymbol{g}_{y}\leftarrow\arg\min_{\boldsymbol{g}_{x},\boldsymbol{g}_{y}}\:||\boldsymbol{\tilde{u}}_{p}-\boldsymbol{E_{g}}^{H}\boldsymbol{E_{g}}\boldsymbol{m}||_{2}^{2}$
\end_inset


\end_layout

\begin_layout Itemize
Update the current image estimate: 
\begin_inset Formula $\boldsymbol{w}=\boldsymbol{w}+\boldsymbol{E_{g}}^{H}\boldsymbol{E}_{g}\boldsymbol{m}$
\end_inset


\end_layout

\begin_layout Itemize
Update the target to optimize at next step: 
\begin_inset Formula $\boldsymbol{\tilde{u}_{p+1}}=\boldsymbol{u}-\boldsymbol{w}$
\end_inset


\end_layout

\begin_layout Itemize
Save the gradient temporal forms: 
\begin_inset Formula $\boldsymbol{g}_{x,p},\boldsymbol{g}_{y,p}\leftarrow\boldsymbol{g}_{x},\boldsymbol{g}_{y}$
\end_inset


\end_layout

\begin_layout Plain Layout

\series bold
End
\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:Gradient-optimization"

\end_inset

Gradient optimization
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Please note that the objective function 
\begin_inset Formula $||\boldsymbol{\tilde{u}}_{p}-\boldsymbol{E_{g}}^{H}\boldsymbol{E_{g}}\boldsymbol{m}||_{2}^{2}$
\end_inset

 is differentiable in the gradient variables 
\begin_inset Formula $\boldsymbol{g}_{x/y}$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Multi-image training
\end_layout

\begin_layout Standard
So far we assumed that the training process is performed on a single input
 magnetization vector 
\begin_inset Formula $\boldsymbol{m}$
\end_inset

.
 To make the training robust to plethora of possible images, the objective
 can be extended to: 
\begin_inset Formula $\sum_{l}^{L}||\boldsymbol{\tilde{u}}_{p,l}-\boldsymbol{E_{g}}^{H}\boldsymbol{E_{g}}\boldsymbol{m_{l}}||_{2}^{2}$
\end_inset

, here 
\shape italic
L 
\shape default
is the total number of images in the dataset.
\end_layout

\begin_layout Subsubsection
Gradient forms reparameterization
\end_layout

\begin_layout Standard
It is easy to see that B0 field induced by the gradients is the time integral
 over the gradient waveform times the ramp constant.
 Possibly more efficient parameterization of the gradient forms can be obtained
 by introducing the variable 
\begin_inset Formula $h:g_{x}=\int_{t1}^{Ta}h_{x}(t)dt$
\end_inset

.
 The variable is essentially a derivative of the gradient temporal forms.
 The desirable property of learned gradient forms is that the derivative
 is non-zero only at a small subset of time points, which promotes less
 load on the gradient amplifier system and results in more smooth trajectories.
 Given such reparameterization, the optimization problem can be reformulated
 as: 
\begin_inset Formula ${\displaystyle \boldsymbol{h}_{x},\boldsymbol{h}_{y}\leftarrow\arg\min_{\boldsymbol{h}_{x},\boldsymbol{h}_{y}}\:||\boldsymbol{\tilde{u}}_{p}-\boldsymbol{E_{h}}^{H}\boldsymbol{E_{h}}\boldsymbol{m}||_{2}^{2}}+\lambda(|\boldsymbol{h}_{x}|_{1}+|\boldsymbol{h}_{y}|_{1})$
\end_inset

, where the sparsity penalty is put on the derivative.
\end_layout

\begin_layout Subsection
Role of the initializations
\end_layout

\begin_layout Standard
The proposed objective function is non-linear due to appearence of the optimized
 variable as an argument in complex exponentials.
 As usual, when optimizing highly non-linear objective the choice of the
 initializations is crucial for convergence to the desired solution.
 In order to obtain good initialization we can potentially benefit from
 the following insight: since it is known [REF] that natural images(and
 medical images as its subset) have scale invariant statistics it is reasonable
 to assume that imaging primitives (basic successfull gradient forms) will
 be prefered in both low resolution and high resolution imaging.
 This allows to initially consider the problem where the number of spins/spatial
 FOV/resolution is low resulting in smaller image matrices, and shorter
 readout signals necessary for reconstruction.
 In such settings, also the gradient variables are expected to be of lower
 dimensionality, which allows to explore the objective more efficiently
 by multiple random restart initializations.
 Having learnt the gradient primitives for such simplified version of the
 problem one can train a regression neural network that is able to evaluate
 successfull vs.
 poor choices of the gradient forms in low resolution regime.
 Such auxiliary network can be then used to efficiently filter potentially
 degenerate initializations that are to be used in higher resolution settings.
\end_layout

\begin_layout Subsection
Optimization + NN reconstruction
\end_layout

\begin_layout Standard
The algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:Gradient-optimization"
plural "false"
caps "false"
noprefix "false"

\end_inset

 can be relaxed by replacing the adjoint operator 
\begin_inset Formula $\boldsymbol{E_{g}}^{H}$
\end_inset

 with a multi-layer perceptron 
\begin_inset Formula $NN_{\theta}$
\end_inset

 parameterized by the weights 
\begin_inset Formula $\theta$
\end_inset

.
 The perceptron can be composed of a concatenation of global layers (fully
 connected) and local ones (convolutional).
 We consider the following joint training process:
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
placement h
wide false
sideways false
status open

\begin_layout Plain Layout

\series bold
Input:
\series default
 Target image 
\begin_inset Formula $\boldsymbol{u}$
\end_inset

 of size 
\begin_inset Formula $N=N_{x}\cdot N_{y}$
\end_inset

; 
\begin_inset Formula $ramp_{x/y}$
\end_inset

 spatial gradient forms; 
\begin_inset Formula $P$
\end_inset

 total number of repetitions.
\end_layout

\begin_layout Plain Layout

\series bold
Output:
\series default
 Temporal gradient forms 
\begin_inset Formula $\boldsymbol{g}_{x/y,p}$
\end_inset

 at each repetition.
 The reconstruction weights 
\begin_inset Formula $\theta_{p}$
\end_inset


\end_layout

\begin_layout Itemize
Initialize 
\begin_inset Formula $\boldsymbol{\tilde{u}_{1}=}\boldsymbol{u}$
\end_inset

: the target to optimize at sequence repetition
\shape italic
 
\begin_inset Formula $p=1$
\end_inset

.
\end_layout

\begin_layout Itemize
Initialize 
\shape italic

\begin_inset Formula $\boldsymbol{w}=0$
\end_inset

: 
\shape default
the reconstructed image.
\end_layout

\begin_layout Itemize
Assume 
\begin_inset Formula ${\displaystyle \boldsymbol{E_{g}=}exp(i(\boldsymbol{D}\boldsymbol{g}_{x}ramp_{x}^{T}+\boldsymbol{D}\boldsymbol{g}_{y}ramp_{y}^{T}))}$
\end_inset


\end_layout

\begin_layout Plain Layout

\series bold
For
\series default
 
\begin_inset Formula $p\leftarrow1...P$
\end_inset

 
\series bold
do
\end_layout

\begin_layout Itemize
Find gradient forms 
\begin_inset Formula $\boldsymbol{g}_{x},\boldsymbol{g}_{y},\theta\leftarrow\arg\min_{\boldsymbol{g}_{x},\boldsymbol{g}_{y},\theta}\:||\boldsymbol{\tilde{u}}_{p}-NN_{\theta}\boldsymbol{E_{g}}\boldsymbol{m}||_{2}^{2}$
\end_inset


\end_layout

\begin_layout Itemize
Update the current image estimate: 
\begin_inset Formula $\boldsymbol{w}=\boldsymbol{w}+NN_{\theta}\boldsymbol{E}_{g}\boldsymbol{m}$
\end_inset


\end_layout

\begin_layout Itemize
Update the target to optimize at next step: 
\begin_inset Formula $\boldsymbol{\tilde{u}_{p+1}}=\boldsymbol{u}-\boldsymbol{w}$
\end_inset


\end_layout

\begin_layout Itemize
Save the gradient temporal forms: 
\begin_inset Formula $\boldsymbol{g}_{x,p},\boldsymbol{g}_{y,p}\leftarrow\boldsymbol{g}_{x},\boldsymbol{g}_{y}$
\end_inset


\end_layout

\begin_layout Itemize
Save neural network weights: 
\begin_inset Formula $\theta_{p}\leftarrow\theta$
\end_inset


\end_layout

\begin_layout Plain Layout

\series bold
End
\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:Gradient-optimization-NN-learning"

\end_inset

Gradient optimization coupled with reconstruction learning
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
It is far from clear how the objective function 
\begin_inset Formula $||\boldsymbol{\tilde{u}}_{p}-NN_{\theta}\boldsymbol{E_{g}}\boldsymbol{m}||_{2}^{2}$
\end_inset

 better be optimized.
 One possibility is to find the partial derivatives w.r.t.
 to both gradients and NN weights 
\begin_inset Formula $\boldsymbol{g}_{x},\boldsymbol{g}_{y},\theta$
\end_inset

 and then make a global gradient step.
 Or one can employ coordinate descent and at each step alternate between
 optimizing for 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{g}_{x},\boldsymbol{g}_{y}$
\end_inset

.
\end_layout

\begin_layout Standard
One problematic aspect of the above formulation is that the network need
 to learn to implicitly perform the inverse Fourier transforms.
 These being global operations in spatial domain would require expensive
 fully connected layers in the network.
 A possible way to make the network fully convolutional is to change the
 objective to: 
\begin_inset Formula $||\boldsymbol{\tilde{u}}_{p}-NN_{\theta}\boldsymbol{E_{g}}^{H}\boldsymbol{E_{g}}\boldsymbol{m}||_{2}^{2}$
\end_inset

, where the role of the network would be to rectify and map to the manifold
 of the plausible images the output of the adjoing gradient operator.
\end_layout

\end_body
\end_document
